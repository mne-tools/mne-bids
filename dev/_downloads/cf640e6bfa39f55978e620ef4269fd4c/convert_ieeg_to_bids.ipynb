{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n.. currentmodule:: mne_bids\n\n# 08. Convert iEEG data to BIDS format\n\nIn this example, we use MNE-BIDS to create a BIDS-compatible directory of iEEG\ndata. Specifically, we will follow these steps:\n\n1. Download some iEEG data.\n\n2. Load the data, extract information, and save in a new BIDS directory.\n\n3. Check the result and compare it with the standard.\n\n4. Cite MNE-BIDS.\n\n5. Repeat the process for the ``fsaverage`` template coordinate space.\n\nThe iEEG data will be written by :func:`write_raw_bids` with\nthe addition of extra metadata elements in the following files:\n\n- the sidecar file ``ieeg.json``\n- ``electrodes.tsv``\n- ``coordsystem.json``\n- ``events.tsv``\n- ``channels.tsv``\n\nCompared to EEG data, the main differences are within the\n``coordsystem.json`` and ``electrodes.tsv`` files.\nFor more information on these files,\nrefer to the `iEEG part of the BIDS specification`_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Adam Li <adam2392@gmail.com>\n#          Stefan Appelhoff <stefan.appelhoff@mailbox.org>\n#          Alex Rockhill <aprockhill@mailbox.org>\n#\n# License: BSD-3-Clause"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os.path as op\nimport shutil\n\nimport mne\nimport nibabel as nib\nimport numpy as np\nfrom nilearn.plotting import plot_anat\n\nfrom mne_bids import (\n    BIDSPath,\n    convert_montage_to_mri,\n    convert_montage_to_ras,\n    get_anat_landmarks,\n    print_dir_tree,\n    read_raw_bids,\n    search_folder_for_text,\n    template_to_head,\n    write_anat,\n    write_raw_bids,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Download the data\n\nFirst, we need some data to work with. We will use the\ndata downloaded via MNE-Python's ``datasets`` API:\n:func:`mne.datasets.misc.data_path`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "misc_path = mne.datasets.misc.data_path()\n\n# The electrode coords data are in the tsv file format\n# which is easily read in using numpy\nraw = mne.io.read_raw_fif(op.join(misc_path, \"seeg\", \"sample_seeg_ieeg.fif\"))\nraw.info[\"line_freq\"] = 60  # specify power line frequency as required by BIDS\nsubjects_dir = op.join(misc_path, \"seeg\")  # Freesurfer recon-all directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When the locations of the channels in this dataset were found in\n`Locating Intracranial Electrode Contacts <tut-ieeg-localize>`,\nthe T1 was aligned to ACPC. So, this montage is in an\n[ACPC-aligned coordinate system](https://surfer.nmr.mgh.harvard.edu/fswiki/CoordinateSystems).\nWe can either save the channel positions in the subject's anatomical\nspace (from their T1 image) or we can transform to a template space\nsuch as ``fsaverage``. To save them in the individual space, it is\nrequired that the T1 have been aligned to ACPC and then the channel positions\nbe in terms of that coordinate system. Automated alignment to ACPC has not\nbeen implemented in MNE yet, so if the channel positions are not in\nan ACPC-aligned coordinate system, using a template (like ``fsaverage``)\nis the best option.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# estimate the transformation from \"head\" to \"mri\" space\ntrans = mne.coreg.estimate_head_mri_t(\"sample_seeg\", subjects_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's convert the montage to \"ras\"\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "montage = raw.get_montage()\nmontage.apply_trans(trans)  # head->mri\nconvert_montage_to_ras(montage, \"sample_seeg\", subjects_dir)  # mri->ras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BIDS vs MNE-Python Coordinate Systems\n\nBIDS has many acceptable coordinate systems for iEEG, which can be viewed in\n`appendix VIII`_ of the BIDS specification.\nHowever, MNE-BIDS depends on MNE-Python and MNE-Python does not support all\nthese coordinate systems (yet).\n\nMNE-Python has a few tutorials on this topic:\n\n- `background on FreeSurfer`_\n- `MNE-Python coordinate frames`_\n\nMNE-Python supports using ``mni_tal`` and ``mri`` coordinate frames,\ncorresponding to the ``fsaverage`` and ``ACPC`` (for an ACPC-aligned T1) BIDS\ncoordinate systems respectively. All other coordinate coordinate frames in\nMNE-Python, if written with :func:`mne_bids.write_raw_bids`, must have\nan :attr:`mne_bids.BIDSPath.space` specified, and will be read in with\nthe montage channel locations set to the coordinate frame 'unknown'.\n\n## Step 2: Formatting as BIDS\n\nNow, let us format the `Raw` object into BIDS.\n\nWith this step, we have everything to start a new BIDS directory using\nour data. To do that, we can use :func:`write_raw_bids`\nGenerally, :func:`write_raw_bids` tries to extract as much\nmeta data as possible from the raw data and then formats it in a BIDS\ncompatible way. :func:`write_raw_bids` takes a bunch of inputs, most of\nwhich are however optional. The required inputs are:\n\n- :code:`raw`\n- :code:`bids_basename`\n- :code:`bids_root`\n\n... as you can see in the docstring:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(write_raw_bids.__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us initialize some of the necessary data for the subject.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# There is a subject, and specific task for the dataset.\nsubject_id = \"1\"\ntask = \"motor\"\n\n# get MNE-Python directory w/ example data\nmne_data_dir = mne.get_config(\"MNE_DATASETS_MISC_PATH\")\n\n# There is the root directory for where we will write our data.\nbids_root = op.join(mne_data_dir, \"ieeg_bids\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To ensure the output path doesn't contain any leftover files from previous\ntests and example runs, we simply delete it.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>Do not delete directories that may contain important data!</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if op.exists(bids_root):\n    shutil.rmtree(bids_root)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we just need make a :class:`mne_bids.BIDSPath` to save the data.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>By passing ``acpc_aligned=True``, we are affirming that\n             the T1 in this dataset is aligned to ACPC. This is very\n             difficult to check with a computer which is why this\n             step is required.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Now convert our data to be in a new BIDS dataset.\nbids_path = BIDSPath(subject=subject_id, task=task, root=bids_root)\n\n# plot T1 to show that it is ACPC-aligned\n# note that the origin is centered on the anterior commissure (AC)\n# with the y-axis passing through the posterior commissure (PC)\nT1_fname = op.join(subjects_dir, \"sample_seeg\", \"mri\", \"T1.mgz\")\nfig = plot_anat(T1_fname, cut_coords=(0, 0, 0))\nfig.axes[\"x\"].ax.annotate(\n    \"AC\",\n    (2.0, -2.0),\n    (30.0, -40.0),\n    color=\"w\",\n    arrowprops=dict(facecolor=\"w\", alpha=0.5),\n)\nfig.axes[\"x\"].ax.annotate(\n    \"PC\",\n    (-31.0, -2.0),\n    (-80.0, -40.0),\n    color=\"w\",\n    arrowprops=dict(facecolor=\"w\", alpha=0.5),\n)\n\n# write ACPC-aligned T1\nlandmarks = get_anat_landmarks(T1_fname, raw.info, trans, \"sample_seeg\", subjects_dir)\nT1_bids_path = write_anat(T1_fname, bids_path, deface=True, landmarks=landmarks)\n\n# write `raw` to BIDS and anonymize it (converts to BrainVision format)\n#\n# we need to pass the `montage` argument for coordinate frames other than\n# \"head\" which is what MNE uses internally in the `raw` object\n#\n# `acpc_aligned=True` affirms that our MRI is aligned to ACPC\n# if this is not true, convert to `fsaverage` (see below)!\nwrite_raw_bids(\n    raw,\n    bids_path,\n    anonymize=dict(daysback=40000),\n    montage=montage,\n    acpc_aligned=True,\n    overwrite=True,\n)\n\n# check our output\nprint_dir_tree(bids_root)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MNE-BIDS has created a suitable directory structure for us, and among other\nmeta data files, it started an ``events.tsv`` and ``channels.tsv`` file,\nand created an initial ``dataset_description.json`` file on top!\n\nNow it's time to manually check the BIDS directory and the meta files to add\nall the information that MNE-BIDS could not infer. For instance, you must\ndescribe ``iEEGReference`` and ``iEEGGround`` yourself.\nIt's easy to find these by searching for ``\"n/a\"`` in the sidecar files.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "search_folder_for_text(\"n/a\", bids_root)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remember that there is a convenient JavaScript tool to validate all your BIDS\ndirectories called the \"BIDS-validator\", available as a web version and a\ncommand line tool:\n\nWeb version: https://bids-standard.github.io/bids-validator/\n\nCommand line tool: https://www.npmjs.com/package/bids-validator\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Load channels from BIDS-formatted dataset and compare\n\nNow we have written our BIDS directory. We can use\n:func:`read_raw_bids` to read in the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# read in the BIDS dataset to plot the coordinates\nraw2 = read_raw_bids(bids_path=bids_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have to go back to \"head\" coordinates.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>If you were downloading this from ``OpenNeuro``, you would\n          have to run the Freesurfer ``recon-all`` to get the transforms.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "montage2 = raw2.get_montage()\n\n# we need to go from scanner RAS back to surface RAS (requires recon-all)\nconvert_montage_to_mri(montage2, \"sample_seeg\", subjects_dir=subjects_dir)\n\n# this uses Freesurfer recon-all subject directory\nmontage2.add_estimated_fiducials(\"sample_seeg\", subjects_dir=subjects_dir)\n\n# get head->mri trans, invert from mri->head\ntrans2 = mne.transforms.invert_transform(mne.channels.compute_native_head_t(montage2))\n\n# now the montage is properly in \"head\" and ready for analysis in MNE\nraw2.set_montage(montage2)\n\n# get the monage, apply the trans and make sure it's the same\n# note: the head coordinates may differ because they are defined by\n# the fiducials which are estimated; as long as the head->mri trans\n# is computed with the same fiducials, the coordinates will be the same\n# in ACPC space which is what matters\nmontage = raw.get_montage()  # the original montage in 'head' coordinates\nmontage.apply_trans(trans)\nmontage2 = raw2.get_montage()  # the recovered montage in 'head' coordinates\nmontage2.apply_trans(trans2)\n\n# compare with standard\nprint(\n    f\"Recovered coordinate: {montage2.get_positions()['ch_pos']['LENT 1']}\\n\"\n    f\"Saved coordinate: {montage.get_positions()['ch_pos']['LENT 1']}\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Cite mne-bids\nWe can see that the appropriate citations are already written in the README.\nIf you are preparing a manuscript, please make sure to also cite MNE-BIDS\nthere.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "readme = op.join(bids_root, \"README\")\nwith open(readme, encoding=\"utf-8-sig\") as fid:\n    text = fid.read()\nprint(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Store coordinates in a template space\nAlternatively, if your T1 is not aligned to ACPC-space or you prefer to\nstore the coordinates in a template space (e.g. ``fsaverage``) for another\nreason, you can also do that.\n\nHere we'll use the MNI Talairach transform to get to ``fsaverage`` space\nfrom \"mri\" aka surface RAS space.\n``fsaverage`` is very useful for group analysis as shown in\n`tut-working-with-seeg`. Note, this is only a linear transform and so\none loses quite a bit of accuracy relative to the needs of intracranial\nresearchers so it is quite suboptimal. A better option is to use a\nsymmetric diffeomorphic transform to create a one-to-one mapping of brain\nvoxels from the individual's brain to the template as shown in\n`tut-ieeg-localize`. Even so, it's better to provide the coordinates\nin the individual's brain space, as was done above, so that the researcher\nwho uses the coordinates has the ability to tranform them to a template\nof their choice.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>For ``fsaverage``, the template coordinate system was defined\n    so that ``scanner RAS`` is equivalent to ``surface RAS``.\n    BIDS requires that template data be in ``scanner RAS`` so for\n    coordinate frames where this is not the case, the coordinates\n    must be converted (see below).</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# ensure the output path doesn't contain any leftover files from previous\n# tests and example runs\nif op.exists(bids_root):\n    shutil.rmtree(bids_root)\n\n# load our raw data again\nraw = mne.io.read_raw_fif(op.join(misc_path, \"seeg\", \"sample_seeg_ieeg.fif\"))\nraw.info[\"line_freq\"] = 60  # specify power line frequency as required by BIDS\n\n# get Talairach transform\nmri_mni_t = mne.read_talxfm(\"sample_seeg\", subjects_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's convert the montage to MNI Talairach (\"mni_tal\").\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "montage = raw.get_montage()\nmontage.apply_trans(trans)  # head->mri\nmontage.apply_trans(mri_mni_t)\n\n# write to BIDS, this time with a template coordinate system\nwrite_raw_bids(\n    raw, bids_path, anonymize=dict(daysback=40000), montage=montage, overwrite=True\n)\n\n# read in the BIDS dataset\nraw2 = read_raw_bids(bids_path=bids_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MNE-Python uses ``head`` coordinates with a ``head -> mri`` ``trans`` so we\nneed to make sure to get our data in this form. As shown below, the montage\nis in the ``mni_tal`` coordinate frame but doesn't have fiducials. The\n``head`` coordinate frame is defined based on the fiducial points so we need\nto add these. Fortunately, there is a convenient function\n(:func:`mne_bids.template_to_head`) that loads stored fiducials and takes\ncare of the transformations. Once this function is applied, you can use\nthe ``raw`` object and the ``trans`` as in any MNE example\n(e.g. `tut-working-with-seeg`).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# use `coord_frame='mri'` to indicate that the montage is in surface RAS\n# and `unit='m'` to indicate that the units are in meters\ntrans2 = template_to_head(raw2.info, space=\"fsaverage\", coord_frame=\"mri\", unit=\"m\")[1]\n# this a bit confusing since we transformed from mri->mni and now we're\n# saying we're back in 'mri' but that is because we were in the surface RAS\n# coordinate frame of `sample_seeg` and transformed to 'mni_tal', which is the\n# surface RAS coordinate frame for `fsaverage`: since MNE denotes surface RAS\n# as 'mri', both coordinate frames are 'mri', it's just that 'mni_tal' is 'mri'\n# when the subject is 'fsaverage'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check that we can recover the original coordinates from the BIDS\ndataset now that we are working in the ``head`` coordinate frame with a\n``head -> mri`` ``trans`` which is the setup MNE-Python is designed around.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# check that we can recover the coordinates\nprint(\n    \"Recovered coordinate head: {recovered}\\n\"\n    \"Original coordinate head:  {original}\".format(\n        recovered=raw2.info[\"chs\"][0][\"loc\"][:3], original=raw.info[\"chs\"][0][\"loc\"][:3]\n    )\n)\n\n# check difference in trans\nprint(\n    f\"Recovered trans:\\n{trans2['trans'].round(3)}\\n\"\n    # combine head->mri with mri->mni to get head->mni\n    # and then invert to get mni->head\n    \"Original trans:\\n\"\n    f\"{np.linalg.inv(np.dot(trans['trans'], mri_mni_t['trans'])).round(3)}\"\n)\n\n# ensure that the data in MNI coordinates is exactly the same\n# (within computer precision)\nmontage2 = raw2.get_montage()  # get montage after transformed back to head\nmontage2.apply_trans(trans2)\nprint(\n    f\"Recovered coordinate: {montage2.get_positions()['ch_pos']['LENT 1']}\\n\"\n    f\"Original coordinate: {montage.get_positions()['ch_pos']['LENT 1']}\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see the coordinates stored in the ``raw`` object are slightly off.\nThis is because the ``head`` coordinate frame is defined by the fiducials\n(nasion, left and right pre-auricular points), and, in the first case,\nthe fiducials were found on the individual anatomy and then transformed\nto MNI space, whereas, in the second case, they were found directly on\nthe template brain (this was done once for the template so that we could\njust load it from a file). This difference means that there are slightly\ndifferent head->mri transforms. Once these transforms are applied, however,\nthe positions are the same in MNI coordinates which is what is important.\n\nAs a final step, let's go over how to assign coordinate systems that are\nnot recognized by MNE-Python. Many template coordinate systems are allowed by\nBIDS but are not used in MNE-Python. For these templates, the fiducials have\nbeen found and the transformations have been pre-computed so that we can\nget our coordinates in the ``head`` coordinate frame that MNE-Python uses.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>As of this writing, BIDS accepts channel coordinates in reference to the\n    the following template spaces: ``ICBM452AirSpace``,\n    ``ICBM452Warp5Space``, ``IXI549Space``, ``fsaverage``, ``fsaverageSym``,\n    ``fsLR``, ``MNIColin27``, ``MNI152Lin``,\n    ``MNI152NLin2009[a-c][Sym|Asym]``, ``MNI152NLin6Sym``,\n    ``MNI152NLin6ASym``, ``MNI305``, ``NIHPD``, ``OASIS30AntsOASISAnts``,\n    ``OASIS30Atropos``, ``Talairach`` and ``UNCInfant``. As discussed above,\n    it is recommended to share the coordinates in the individual subject's\n    anatomical reference frame so that researchers who use the data can\n    transform the coordinates to any of these templates that they choose.</p></div>\n\nBIDS requires that the template be stored in ``scanner RAS`` coordinates\nso first we'll convert our original data to ``scanner RAS`` and then\nconvert it back. Just in case the template electrode coordinates are\nprovided in voxels or the unit is not specified, these options are able\nto be overridden in :func:`mne_bids.template_to_head` for ease of use.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>If no coordinate frame is passed to :func:`mne_bids.template_to_head`\n    it will infer ``voxels`` if the coordinates are only positive and\n    ``scanner RAS`` otherwise. Be sure not to use the wrong coordinate\n    frame! ``surface RAS`` and ``scanner RAS`` are quite similar which\n    is especially confusing, but, fortunately, in most of the Freesurfer\n    template coordinate systems ``surface RAS`` is identical to\n    ``scanner RAS``. ``surface RAS`` is a Freesurfer coordinate frame so\n    it is most likely to be used with Freesurfer template coordinate\n    systems). This is the case for ``fsaverage``, ``MNI305`` and\n    ``fsaverageSym`` but not ``fsLR``.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The template should be in scanner RAS:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# ensure the output path doesn't contain any leftover files from previous\n# tests and example runs\nif op.exists(bids_root):\n    shutil.rmtree(bids_root)\n\n# get a template mgz image to transform the montage to voxel coordinates\nsubjects_dir = op.join(mne.datasets.sample.data_path(), \"subjects\")\ntemplate_T1 = nib.load(op.join(subjects_dir, \"fsaverage\", \"mri\", \"T1.mgz\"))\n\n# get voxels to surface RAS and scanner RAS transforms\nvox_mri_t = template_T1.header.get_vox2ras_tkr()  # surface RAS\nvox_ras_t = template_T1.header.get_vox2ras()  # scanner RAS\n\nraw = mne.io.read_raw_fif(\n    op.join(misc_path, \"seeg\", \"sample_seeg_ieeg.fif\")  # load our raw data again\n)\nmontage = raw.get_montage()  # get the original montage\nmontage.apply_trans(trans)  # head->mri\nmontage.apply_trans(mri_mni_t)  # mri->mni\npos = montage.get_positions()\nch_pos = np.array(list(pos[\"ch_pos\"].values()))  # get an array of positions\n# mri -> vox and m -> mm\nch_pos = mne.transforms.apply_trans(np.linalg.inv(vox_mri_t), ch_pos * 1000)\nch_pos = mne.transforms.apply_trans(vox_ras_t, ch_pos)\n\nmontage_ras = mne.channels.make_dig_montage(\n    ch_pos=dict(zip(pos[\"ch_pos\"].keys(), ch_pos)), coord_frame=\"ras\"\n)\n\n# specify our standard template coordinate system space\nbids_path.update(datatype=\"ieeg\", space=\"fsaverage\")\n\n# write to BIDS, this time with a template coordinate system in voxels\nwrite_raw_bids(\n    raw, bids_path, anonymize=dict(daysback=40000), montage=montage_ras, overwrite=True\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's load our data and convert our montage to ``head``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "raw2 = read_raw_bids(bids_path=bids_path)\ntrans2 = template_to_head(  # unit='auto' automatically determines it's in mm\n    raw2.info, space=\"fsaverage\", coord_frame=\"ras\", unit=\"auto\"\n)[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check to make sure again that the original coordinates from the BIDS\ndataset were recovered.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "montage2 = raw2.get_montage()  # get montage after transformed back to head\nmontage2.apply_trans(trans2)  # apply trans to go back to 'mri'\nprint(\n    f\"Recovered coordinate: {montage2.get_positions()['ch_pos']['LENT 1']}\\n\"\n    f\"Original coordinate: {montage.get_positions()['ch_pos']['LENT 1']}\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In summary, as we saw, these standard template spaces that are allowable by\nBIDS are quite complicated. We therefore only cover these cases because\ndatasets are allowed to be in these coordinate systems, and we want to be\nable to analyze them with MNE-Python. BIDS data in a template coordinate\nspace doesn't allow you to convert to a template of your choosing so it is\nbetter to save the raw data in the individual's ACPC space. Thus, we\nrecommend, if at all possible, saving BIDS iEEG data in ACPC coordinate space\ncorresponding to the individual subject's brain, not in a template\ncoordinate frame.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}